\documentclass[a4paper,twocolumn]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{fullpage}

\usepackage[small]{titlesec}
% patch titlesec bug of not showing (sub)title numbering http://tex.stackexchange.com/a/300259
\usepackage{etoolbox}
\makeatletter
\patchcmd{\ttlh@hang}{\parindent\z@}{\parindent\z@\leavevmode}{}{}
\patchcmd{\ttlh@hang}{\noindent}{}{}{}
\makeatother

\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

\newcommand{\ar}{\rightarrow}

\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}
\makeatother

\title{Learning without Forgetting $-$ summary}
\author{Matěj Nikl}

\begin{document}
\maketitle
\noindent
This paper is about a method called Learning without Forgetting (LwF), which focuses on the task of incremental learning of new capabilities without forgetting the old ones. On top of that, this method can accomplish this task without the need for the training data the old capabilities were trained with.

It resembles the combination of \textit{Knowledge Distilling} and transfer learning strategy \textit{fine-tuning}.

\section{The Architecture}
The LwF's view on a model consists of:
\begin{itemize}
    \item shared parameters $\theta_s$
    \item task-specific parameters for previously learned tasks $\theta_o$
    \item randomly initialized task-specific parameters for new tasks $\theta_n$
\end{itemize}
It is useful to think of $\theta_o$ and $\theta_n$ as classifiers (fully connected layers) that operate on features parameterized by $\theta_s$ (see Figure \ref{fig:methods})

\section{Learning without Forgetting}
Given a (C)NN with $\theta_s$ and $\theta_o$, the goal is to add $\theta_n$ for new tasks and learn all the parameters that work well on both old and new tasks, using only labeled data for the new tasks:
\begin{enumerate}
    \item record responses (probability distributions over classes) $y_o$ for each example of the new dataset from the original network (defined by $\theta_s$ and $\theta_o$)
    \item add new fully connected classifiers $\theta_n$ on top of $\theta_s$, that will compute the new tasks' class probability distributions
    \item fine-tune $\theta_n$ using supervised cross-entropy loss \ref{ssec:CEloss} (sum over losses if multiple new tasks) until convergence (having $\theta_s$ (and $\theta_o$) frozen)
    \item train all parameters jointly using a (weighted) sum of supervised cross-entropy loss and Knowledge Distillation loss \ref{ssec:KDloss} (sum over losses if multiple old tasks) until convergence
\end{enumerate}

\subsection{Cross-entropy loss}
\label{ssec:CEloss}
\begin{equation*}
    \mathcal{L}_{\mathrm{new}}(y_n, \hat{y}_n) = -y_n \log\hat{y}_n
\end{equation*}

\subsection{Knowledge Distillation loss}
\label{ssec:KDloss}
The Knowledge Distillation loss adds the objective of keeping the outputs of the old task classifiers for all of the new dataset inputs the same as they were, before the learning of the new tasks began.
\begin{align*}
    \mathcal{L}_{\mathrm{old}}(y_o, \hat{y}_o) &= D_{\mathrm{KL}}(y_o^\prime\|\hat{y}_o^\prime) \\
                &= - \sum_{i=1}^l y_o^{\prime(i)} \log(\hat{y}_o^{\prime(i)}) \\
                &= - \sum_{i=1}^l \left( \frac{e^{y_o^{(i)}/T}}{\sum_j e^{y_o^{(j)}/T}} \log \left( \frac{e^{\hat{y}_o^{(i)}/T}}{\sum_j e^{\hat{y}_o^{(j)}/T}} \right) \right)
\end{align*}
where $l$ is the number of classes and $T$ is the temperature of the softmax function. The recommended setting is $T > 1$, because then the weight of smaller logit is bigger and encourages the network to better encode similarities among classes. % The used setting in this paper was $T = 2$.

\section{Principles of modularization}
Each of the $\theta_o$ output classifiers can be seen as a module. There is no need for the network to always output all $\theta_o$ outputs $-$ only a selected output classifier for the selected task at hand can be connected to produce the desired output. The output classifiers, however, ale strongly tied to the shared parameters $\theta_s$ with whom they have been trained with. The output modules are thus not transferable to a different model with different $\theta_s$.

\section{Principles of growing}
For each new task a new output layer $\theta_n$ is crated. The whole set of parameters $\theta$ is adjusted:
\begin{itemize}
    \item $\theta_s$ and $\theta_o$ in such a way that they allow for the new task to be learned, while also retaining the old knowledge
    \item $\theta_n$ in such a way that it performs well on the new task
\end{itemize}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.6\textwidth]{methods.png}
    \caption{Illustration of LwF method (e) and other methods (b-d).}
    \label{fig:methods}
\end{figure*}
{
\renewcommand{\arraystretch}{1.25}
\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
    \begin{tabular}{r *{20}{c}}
        \thickhline
                         & \multicolumn{2}{c}{ImageNet$\ar$VOC} && \multicolumn{2}{c}{ImageNet$\ar$CUB} && \multicolumn{2}{c}{ImageNet$\ar$Scenes} && \multicolumn{2}{c}{Places2$\ar$VOC} && \multicolumn{2}{c}{Places2$\ar$CUB} && \multicolumn{2}{c}{Places2$\ar$Scenes} && \multicolumn{2}{c}{ImageNet$\ar$MNIST} \\
                         \cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12} \cline{14-15} \cline{17-18} \cline{20-21}
                         & old  & new  && old  & new  && old  & new  && old  & new  && old  & new   && old  & new  && old  & new   \\
        \hline
        LwF (ours)       & 56.5 & 75.8 && 55.1 & 57.5 && 55.9 & 64.5 && 43.3 & 72.1 && 38.4 & 41.7  && 43.0 & 75.3 && 52.1 & 99.0 \\
        \hline
        fine-tuning      & -1.4 & -0.3 && -5.1 & -1.5 && -3.4 & -1.0 && -1.8 & -0.1 && -9.1 & -0.8  && -4.1 & -0.8 && -4.9 & 0.2 \\
        feat. extraction & 0.5  & -1.1 && 2.0  & -5.3 && 1.2  & -3.7 && -0.2 & -3.9 && 4.7  & -19.4 && 0.2  & -0.5 && 5.0  & -0.8 \\
        \hline
        joint training   & 0.2  & 0.0  && 0.5  & -0.9 && 0.5  & -0.6 && -0.1 & 0.1  && 3.3  & -0.2  && 0.2  & 0.1  && 4.7  & 0.2 \\
        \thickhline
    \end{tabular}}
    \caption{Performance for the single new task scenario using AlexNet structure. The difference
    of methods’ performance with LwF is reported to facilitate comparison.}
\end{table*}
}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{gradual.png}
    \caption{Performance of each task when gradually adding new tasks to a pre-trained network. The $x$-axis labels indicate the new task added to the network each time. Error bars shows $\pm2$ standard deviations for 3 runs with different $\theta_n$ random initializations. Markers are jittered horizontally for visualization, but line plots are not jittered to facilitate comparison.}
    \label{fig:table}
\end{figure*}
\end{document}
